# 1) Overview of Reinforcement Learning


## 강화학습이란?
### Learn to make good sequences of decisions - 순차적 선택을 잘 하도록 학습하는 것 

- **sequences of decisions** : 하나의 결정이 아닌, 연속적 결정 
- **good** : 최적의 결정
- **learn** : 경험을 통한 학습

불확실성 안에서 좋은 결정을 내리는 법을 학습하는 것! 

## 강화학습의 적용 예

- Atari
- Robotics
- Healthcare
- NLP,Vision,etc..


## 강화학습 개념

Optimization : 최적화
- 가장 좋은 결과를 내는 방법을 찾는 것

Delayed Consequences : 지연된 보상
- 현재 선택이 지연도니 피드백을 받는 것 (알고리즘이 당장의 보상과 함께 미래의 보상을 고려해야 함)

Exploration : 탐험
- 사전 준비된 데이터가 아닌 학습의 결과로서 데이터를 쌓는 것

Generalization : 일반화
- 이전에 학습되지 않은 상황에서도 잘 동작하는 것

### 기존 머신러닝 기법 또한 Optimization, Delayed Consequences, Generalization을 포함하는 경우가 많음. 그러나 Exploration은 오직 강화 학습에만 적용되는 개념




# 2) Introduction to Sequential Decision making under uncertainty

## 용어 정리

- Agent : 강화학습을 수행하는 주체
- Action : 행동
- Observation : Agent가 보고 관찰하는 것
- Reward : Action에 따른 보상
- World : 학습시키고자 하는 환경 

## Sequential Decision Making?

-> Agent가 World와 상호작용하며 Observation을 바탕으로 Reward를 주는 것을 반복하는 구조

핵심 목표는 미래까지의 총 기대보상을 최대화 하는것 
따라서 즉각적인 보상과 장기적 보상 사이의 전략적 행동을 요구

## Example

### Web Advertising

- Goal : 고객의 광고 클릭 횟수 최대화 
- Action : 노출할 광고를 고르는 것
- Observation : 광고 클릭 횟수, 시간
- Reward : 광고 클릭시 + 1 

### Blood Pressure

- Goal : 혈압을 정상치로 만드는 것
- Action : 운동 또는 약 처방
- Observation : 혈압 상태 및 건강 상태 
- Reward : 혈압이 정상치 + 1 , 약 부작용 -0.05

### Tutoring

- Goal : 학생의 성취 수준 향상 
- Action : Teaching 방법을 제시
- Observation : 학생의 문제 풀이 결과
- Reward : 문제를 맞추면 + 1 , 틀리면 -1 

#### Reward Hacking
Agent는 Reward 극대화를 위해 쉬운 문제만 출제할 것. 이는 Goal과 다른 결과를 유도할 수 있음
이를 Reward Hacking이라고 부름. Reward Function을 잘 제시하는 것이 중요





## Sequential Decision Process

### Agent & World
<img width="500px" alt='캡쳐1' src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcK7GS7%2FbtqyOnH4VAR%2FYanSx0ZuWMw7XlckyGtYz1%2Fimg.png">

각 time step 마다 Agent는 action a를, World는 observation o를, Reward r을 반환함. 이를 바탕으로 Agent는 다음 action을 결정함

### History 

<img width="500px" alt='캡쳐2' src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FEz3aV%2FbtqyOn89lXz%2F6RVSkmZpb8a9CV3Xsokqi0%2Fimg.png">

History는 Action과 Obs의 총 집합. Agent는 History를 바탕으로 새로운 Action을 결정 

### State
<img width="500px" alt='캡쳐3' src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbfKlhr%2FbtqyMrErxsP%2Fc65qWpkLaGrWIY1QXDwghk%2Fimg.png">

State는 History의 일부분. 즉 State는 History의 함수. Agent는 State를 바탕으로 새로운 Action을 결정 

State는 Observation과 다름. Observation은 관찰되는 상황 그 자체이고, State는 결정을 내릴 때 사용되는 상황 전체 이기에, 문제 정의에 따라 달라질 수 있음. 

State에는 다음 두가지가 존재. 

- World State : Real World의 State로 Agent는 이를 전부 알 수 없음

- Agent State : Agent가 알고 있는 State. World State의 부분 집합. Agent가 결정을 내릴 때 사용하는 State


## Markov Assumption

### "State가 모든 History를 반영한다면, 그 State만으로 미래를 예측하고 새로운 결정을 내릴 수 있다"

위 가정이 성립한다면 Agent에게 주어진 State만으로 효과적으로 학습한다는 가정이 성립할 수 있음.
현재 관찰한 Obs가 State가 되고, 결정은 현재에만 의존하게 됨. 미래는 과거와 독립적으로 변할 수 있음. 

<img width="500px" alt='캡쳐4' src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbhQDJ5%2FbtqyOVduLRL%2F7LXpEdBPwMgDaV56b7nnzK%2Fimg.png">

그러나 대부분의 예시에서 Markov Assumption은 성립하지 않음. 그럼 왜 중요한가? 

**State를 어떻게 설정하냐**에 따라 Markov 가정은 성립할 수 있기 때문. State를 Full of History로 설정하면 Markov는 성립. 그러나 이는 비용상의 문제가 있으므로 도메인마다의 State 설정이 필요. 

## Markov Decision Process 

### Full Obervability : MDP 
MDP는 Agent가 모든 상황을 다 관찰할 수 있는 경우 즉 Agent State = World State = State

State는 현재 관찰되는 Obs로 설정하여도 괜찮음 

### Partial Obervability : POMDP
Agent State가 World State 보다 작은 경우 (대부분의 케이스에 해당 Ex:포커, Health care.. 등)


### Typles of Sequentail Decision Processes

- Bandits : 이전 action이 다음 action에 영향을 주지 않음. 즉 과거와 미래가 독립이기에 Markov 가정이 성립. 

- MDPs and POMDPs : 이전 action이 다음 action에 영향을 줌. 현재의 obs 만으로 state를 설정하면 결정을 내리는데 정보가 부족할 수 있음

- How the World Changes :
  - Deterministic : Action을 취했을 때 결과 값이 고정 (Robotics 등)
  - Stochastic : Action을 취했을 때 결과 값이 확률적으로 결정됨 (보통 현실의 문제들이 이에 해당)


# 3) Further about RL Algorithm 

## RL Algorithm Components

<img width="500px" alt='캡쳐5' src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FkNf8D%2FbtqyK8lmXWs%2F2DabeW8lJKYD4qFg4XJrW1%2Fimg.png">

위의 Mars Rover 문제로 S1에 +1, S7에 +10, 나머지에 0의 보상이 있다고 가정.

### Model
Agent의 Action에 따라 World가 어떻게 바뀔지에 대한 예측을 출력하는 함수 
- Transition / Dynamics Model : Agent의 다음 State를 예측
- Reward Model : 그 순간 즉각 보상을 예측
  
Reward의 보상체계와 다르더라도 많은 경우에 사용 가능

### Policy
Agent가 현재 State에서 어떤 Action을 취할지를 출력하는 함수
- Deterministic Policy : 하나의 Action을 취할 확률이 1
- Stochastic Policy : 여러개의 Action을 취할 확률이 각각 0~1

### Value
현재 상태에서 받을 수 있을 것이라고 기대되는 보상의 총 합. 미래의 모든 보상까지 고려한 값 (waiting의 개념을 포함) gamma는 보상에 대한 가중치를 조절하는 파라미터

## Types of RL Agents

### Model-based Agents
World가 어떻게 굴러가는지를 표현. Model이 정책, 가치 함수를 포함하고 있다면 이를 알기 위해 추가적인 연산이 필요.

### Model-free Agents
명시된 가치함수와 정책함수가 존재, Model은 존재하지 않음. 

<img width="500px" alt='캡쳐6' src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcyIMyj%2FbtqyKIf07Et%2FOZvcKo67KSfxbXphUKZPSK%2Fimg.png">

## Key of Learning to Make Sequences of Good Decisions

### AI
- World의 구조와 Rule을 인식
- 학습 과정에서 World를 탐험할 필요 X
- World와의 상호작용 X, 연산을 통해 High reward를 얻을 수 있는 선택지를 결정
- 눈 앞의 상황을 모두 분석해 최적의 수를 놓기만 하면 됨

### RL
- World의 구조를 인식 X
- 탐험을 통해 World를 이해해 나감
- 경험과 탐험을 통해 Policy를 만들어 나감
- 어떤 Action이 가장 Best인지와 더 많은 정보를 얻기 위해 어떤 Action을 취해야하는지 모두 고려해야 함


## Exploration versus Exploitation

- Exploration : 새로운 정보를 얻기 위한 탐험 (폭 넓게 학습하기 위함) 
- Exploitation : 이전의 경험에 대한 활용 (Action을 재활용 해 손실을 줄이기 위함) 